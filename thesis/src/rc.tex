\chapter{Reservoir Computing}

\section{Introduction}

\gls{rc} is a bio-inspired artificial recurrent neural network which is based on the \gls{esn} paradigm introduced by Herbert Jaeger in \cite{Jaeger2004}. This computation scheme is well suited for real-time data processing and for chaotic time series prediction\cite{Jaeger2004, JaegerH.2001Tesa, Lukoeviius2012}, and achieves state of the art performances in those domains, as well as in speech recognition\cite{Verstraeten2006, NIPS2010_4056, Jaeger2007}, nonlinear channel equalisation\cite{Jaeger2004} and financial forecasting \cite{financialTimeSeries}.\\

A \gls{rcer} is made of a large ensemble of interconnected neurons, which are merely entities carrying an activation level. The activation level  is updated according to the connection weights of the reservoir, or \emph{synaptic matrix} as it is referred to in the field of neural networks, and with a nonlinear function, called the \textit{activation function}. The nonlinear character is one of the main features making neural networks so powerful. Moreover, with a proper activation function, one can reach a saturation state, which mimics the behaviour of biological neurons. This is traditionally achieved using the \textit{sigmoid} function. Those principles are introduced in \cite[p.227-228]{bishop2006pattern} and in \cite[p.727-728]{russell2010artificial}.\\

